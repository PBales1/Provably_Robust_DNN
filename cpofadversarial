m\documentclass[12pt,oneside,reqno]{amsart}

\usepackage[shortlabels]{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{indentfirst}
\usepackage{biblatex}

\addbibresource{references.bib}

\usepackage[margin=1in]{geometry}
\usepackage{float}

\usepackage{graphicx}
\usepackage{hyperref} 

% Feel free to add any of your own macros, etc here

\title{Creating a Provably Robust Deep Neural Network}  % Feel free to make more descriptive
\author{Benjamin Asch}
\author{Patrick Bales}  % Feel free to delete if unnecessary.
%\author{Author 3}  % Feel free to delete if unnecessary.

\begin{document}

\maketitle

% May want \tableofcontents if you submit a long report with sections
% \section{Section 1}

\begin{center}{\large{Background Research and Literature Review}}
\end{center}
 \hspace{+15pt} The existence of adversarial examples in neural networks poses a large amount of concern when trusting these models with important decision making tasks. Carlini and Wagner show the failure of the latest state of the art method of protecting against adversarial attacks, defensive distillation, in the paper, "Towards Evaluating the Robustness of Neural Networks". Defensive distillation is the process of training a network on the original labels, smoothing out the softmax function with a constant T, and then taking the “distilled” network to be a second network that is trained on the smoothed outputs from the first network. This distillation has proved quite effective against previous methods for generating adversarial examples, including L-BFGS. It was previously thought that this is due to the smoothing of the soft outputs preventing overfitting on the training data, which could get rid of blind spots caused by the nonlinearity of the networks. However, new research indicates that adversarial examples to distilled networks may not exist due to blind spots but due to the local linearity of neural networks, an idea supported by, as Carlini and Wagner show, the fact that distillation does not get rid of all adversarial examples.

\hspace{+15pt} L-BFGS works by minimizing the L2 distance between the original and perturbed image corresponding to a target classification. This corresponds to the problem:
                    \begin{align}
                     \mathrm{min} \; \| x - x'\|_{2}^{2}\\
                     \nonumber \mathrm{s.t.} \; C(x') = l \\ \nonumber x' \in [0, 1]^{n}
                     \end{align}
                    
\hspace{-15pt} $x$ is our original input, and $x'$ our perturbed input. $C(\cdot)$ and $l$ correspond to our classifier and target output respectively. Because this is often a difficult problem to solve, this is often rewritten as: 
                    \begin{align}
                     \mathrm{min}& \; c \cdot \| x - x' \|_{2}^{2} + \mathrm{loss}_{F,l}(x')\\
                   \nonumber \mathrm{s.t.}& \; x' \in [0, 1]^{n}
                    \end{align}
                    
Here, $\mathrm{loss}_{F,l}(\cdot) : \mathbb{R}^{n} \rightarrow \mathbb{R}$. In practice, cross-entropy is often used.\newline

The general form of finding an adversarial example can then be defined as the following:\begin{align}
                        \mathrm{min}& \; \mathcal{D}(x, x + \delta)\\
                        \nonumber \mathrm{s.t.}& \; C(x + \delta) = t\\
                       \nonumber &x + \delta \in [0, 1]^{n}
                    \end{align}

$\mathcal{D}(\cdot)$ is our chosen distance function. [insert number here] analyze and develop adversarial examples for $\mathcal{D}(\cdot)$ as the $l_0$, $l_2$, and $l_{\infty}$ distance functions. The constraint $C(x + \delta) = t$ introduces non-linearity that is difficult for optimization. [insert number here] introduce an objective function, $f(\cdot)$ such that $C(x + \delta) = t \Leftrightarrow f(x + \delta) \leq 0$. This implies the following reformatting:
                \begin{align}
                    \mathrm{min} & \; \mathcal{D}(x, x + \delta) + c \cdot f(x + \delta)\\
                    \nonumber \mathrm{s.t.} & \; x + \delta \in [0, 1]^{n}
                \end{align}
\newpage
\begin{center}
    \large{Finding the Fenchel Conjugates}
    \tiny{6, 12}
\end{center}
We previously derived the dual problem in terms of the Fenchel conjugates of $\boldsymbol{1}_{B_{\epsilon}(\vec{x})}$ and $\boldsymbol{1}_{\mathcal{Z}_j}$, where $\boldsymbol{1}_S$ was defined earlier as
                \begin{align}
                    \boldsymbol{1}_{S}(x) = \begin{cases}
                    0 & x \in S\\
                    +\infty & \mathrm{otherwise}
                    \end{cases}
                \end{align}
                
We now find the values of these conjugates:
                \begin{align}
                \boldsymbol{1}^{*}_{B_{\epsilon}(\vec{x})}(\vec{\nu}) &= \sup_{\vec{y} \in \mathbb{R}^{n}} \left\{ \vec{\nu}^{\top}\vec{y} - \boldsymbol{1}_{B_{\epsilon}(\vec{x})}(\vec{y}) \right\}\\
                \nonumber &= \sup_{\vec{y} \in \mathbb{R}^{n}} \left\{ \vec{\nu}^{\top}\vec{y} \mid \| \vec{y} - \vec{x} \|_{\infty} \leq \epsilon \right\}\\
                \nonumber &= \sup_{\vec{y} \in \mathbb{R}^{n}} \left\{ \vec{\nu}^{\top}\vec{y} \mid |y_i - x_i | = \epsilon, \forall i \in \{1, \dots, n\} \right\}
                \end{align}
This implies that at the supremum:
                \begin{align}
                    &y_i = x_i + \epsilon \cdot \mathrm{sgn}(\nu_i), \; \forall i \in {1, \dots, n}\\
                    &\implies \nu_i y_i = \nu_i x_i + \epsilon \|\nu_i\|_{1}
                \end{align}
Now, continuing from (7):
                \begin{align}
                    \boldsymbol{1}^{*}_{B_{\epsilon}(\vec{x})}(\vec{\nu}) &= \sup_{\vec{y}} \left\{ \vec{\nu}^{\top}\vec{y} \mid y_i = x_i + \epsilon \cdot \mathrm{sgn}(\nu_i), \forall i \in \{1, \dots, n\} \right\}\\
                    \nonumber &= \vec{\nu}^{\top}\vec{x} + \epsilon\|\vec{\nu}\|_{1}
                \end{align}
Finding $\boldsymbol{1}^{*}_{\mathcal{Z}_j}$ is a bit more complicated and requires casework.
\begin{enumerate}[(i)]
    \item For $l_j \leq u_j \leq 0$,
                    
                    \[\mathcal{Z}_j \subseteq \{(\hat{z}, z) : z = 0\}\]
                    \begin{align}
                    \implies \boldsymbol{1}^{*}_{\mathcal{Z}_j}(\hat{\nu}, -\nu) &= \sup_{\hat{z} \in \mathcal{Z}_j} \hat{\nu} \cdot \hat{z}\\
                    \nonumber &= \boldsymbol{1}_{(\hat{\nu} = 0)}\\
                    \nonumber &= \begin{cases}
                        0   &   \nu = 0\\
                        +\infty     &   \mathrm{otherwise}
                    \end{cases}
                    \end{align}
    \item For $0 \leq l_j \leq u_j$,
                    \[\mathcal{Z}_j \subseteq \{(\hat{z}, z) : \hat{z} = z \}\]
                    \begin{align}
                        \implies \boldsymbol{1}^{*}_{\mathcal{Z}_j}(\hat{\nu}, -\nu)
                        &= \sup_{z} -\nu \cdot z + \hat{\nu} \cdot z\\
                        \nonumber &= (\hat{\nu} - \nu)z\\
                        \nonumber &= \boldsymbol{1}_{\hat{\nu} - \nu = 0} = \begin{cases}
                            0   &   \nu = \hat{\nu}\\
                            +\infty     &   \mathrm{otherwise}
                        \end{cases}
                    \end{align}
    \item For $l_j \leq 0 \leq u_j$, the convex relaxation gives us
                    \begin{align}\mathcal{Z}_j \subseteq \{(\hat{z}, z) : z \geq \hat{z} \geq 0, -u\hat{z} + (u - l)z = -ul\}\end{align}
    Because the optimum of a linear program can always be attained at one of the vertices of the feasible polytope, we know that our desired optimum is attained at one of the three vertices on $\hat{\mathcal{Z}}$. This implies that our optimal point $(y, \hat{y})$ is either $(0, 0)$ or lies on the line
                    \begin{align}
                        -u_j\hat{y} + (u_j - l_j)y = -u_{j}l_{j}
                    \end{align}
    connecting the remaining two vertices. Note that this implies that $\boldsymbol{1}^{*}_{\mathcal{Z}_j}(\hat{\nu}, -\nu)$ is at least $0$. We will now analyze the latter case. \newline
    Rewriting $z$ in terms of $\hat{z}$,
                    \begin{align}
                        z = \frac{-u_{j}l_j + u\hat{z}}{u_j - l_j}
                    \end{align}
                    \begin{align}
                        \implies \boldsymbol{1}^{*}_{\mathcal{Z}_j}(\hat{\nu}, -\nu)
                        &= \sup_{0 < \hat{z} < u} \hat{\nu} \cdot \frac{-ul + u\hat{z}}{u - l} - \nu \cdot \hat{z}\\
                        \nonumber &= \sup_{0 < \hat{z} < u} \frac{\hat{\nu} u_j}{u_j - l_j}\hat{z} - \frac{\hat{\nu} u_j}{u_j - l_j}l_{j} - \nu \cdot \hat{z}\\
                        \nonumber &= \sup_{0 < \hat{z} < u} \frac{-\hat{\nu} u_j}{u_j - l_j}l_{j} + (\frac{\hat{\nu} u_j}{u_j - l_j} - \nu)\hat{z}\\
                        \nonumber &= \begin{cases}
                            -\frac{u_{j}l_j}{u_j - l_j}\hat{\nu} & \frac{\hat{\nu}u_j}{u_j - l_j} - \nu \leq 0\\
                            (\frac{\hat{\nu}u_j}{u_j - l_j} - \nu)u -\frac{u_{j}l_j}{u_j - l_j}\hat{\nu}    &   \frac{\hat{\nu}u_j}{u_j - l_j} - \nu > 0
                        \end{cases}\\
                        \nonumber & \leq \begin{cases}
                            \mathrm{max}(0, -l_{j}\nu)  &   \nu = \frac{\hat{\nu}u_j}{u_j - l_j}\\
                            +\infty     &   \mathrm{otherwise}
                        \end{cases}\\
                         &= \begin{cases}
                            \mathrm{ReLU}(-l_{j}\nu)    &   \nu = \frac{\hat{\nu}u_j}{u_j - l_j}\\
                            +\infty     &   \mathrm{otherwise}
                        \end{cases}
                    \end{align}
\end{enumerate}
\hfill\break

\begin{center}
    \large{The Dual Network} \tiny{7}
\end{center}
\hspace{+15pt} Using the previous expressions[(9), (10), (11), (16)] found for the conjugates of our characteristic functions, we can reformulate the dual problem. In an attempt to reduce the notational burden, we introduce the following sets:

                            \begin{align}
                                \nonumber &S := \{j \in [n_2] \mid l_j \leq 0 \leq u_j\}\\
                                &S^{-} := \{j \in [n_2] \mid l_j \leq u_j \leq 0\}\\
                                \nonumber &S^{+} := \{j \in [n_2] \mid 0 \leq l_j \leq u_j\}
                            \end{align}
Now, applying our previous results:
                            \begin{align}
                                d^{*}(\vec{x}, \vec{c}) &= 
                                 \max_{\vec{\nu}} \quad -\boldsymbol{1}^{*}_{B_{\epsilon}(\vec{x})}(\vec{\hat{\nu}}_1) + \sum_{j=1}^{n_2} -\boldsymbol{1}^{*}_{\mathcal{Z}_j}(\hat{\nu}_{2_j}, -\nu_{2_j}) - \sum_{i=1}^{2}\vec{\nu}^{\top}_{i+1}\vec{b}_i\\
                                 &\nonumber \quad \mathrm{s.t.} \quad \vec{\nu}_3 = -\vec{c}\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_2 = W_{2}^{\top}\vec{\nu}_3\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_1 = W_{1}^{\top}\vec{\nu}_2\\ %break
                                 &= \max_{\vec{\nu}} -\vec{\hat{\nu}}_{1}^{\top}\vec{x} - \epsilon \|\vec{\hat{\nu}}_1 \|_1 + \sum_{j \in S} -\mathrm{ReLU}(-l_{j}\nu_j) - \sum_{i=1}^{2} \vec{\nu}_{i+1}^{\top}\vec{b}_i\\
                                 &\nonumber \quad \mathrm{s.t.} \quad \vec{\nu}_3 = -\vec{c}\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_2 = W_{2}^{\top}\vec{\nu}_3\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_1 = W_{1}^{\top}\vec{\nu}_2\\ 
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = 0 & \forall j \in S^{-}\\
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = \hat{\nu}_{2_j} & \forall j \in S^{+}\\
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = \frac{u_j}{u_j - l_j}\hat{\nu}_{2_j} & \forall j \in S\\ %break
                                 &= \max_{\vec{\nu}} -\vec{\hat{\nu}}_{1}^{\top}\vec{x} - \epsilon \|\vec{\hat{\nu}}_1 \|_1 - \sum_{i=1}^{2} \vec{\nu}_{i+1}^{\top}\vec{b}_i + \sum_{j \in S} \mathrm{ReLU}(l_{j}\nu_j)\\
                                 &\nonumber \quad \mathrm{s.t.} \quad \vec{\nu}_3 = -\vec{c}\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_2 = W_{2}^{\top}\vec{\nu}_3\\
                                 & \nonumber \quad \quad \quad \; \vec{\hat{\nu}}_1 = W_{1}^{\top}\vec{\nu}_2\\ 
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = 0 & \forall j \in S^{-}\\
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = \hat{\nu}_{2_j} & \forall j \in S^{+}\\
                                 & \nonumber \quad \quad \quad \; \nu_{2_j} = \frac{u_j}{u_j - l_j}\hat{\nu}_{2_j} & \forall j \in S
                            \end{align}

Notice that, despite having ReLU non-linearity in both the objective function and constraints, the non-linearity introduced in the dual is formed from the upper bound in (16) that creates a convex outer bound.

\begin{center}
    \large{Finding the bounds}\tiny{8}
\end{center}

\section{Introduction}

Feel free to add more sections/subsections as necessary.

\section{Methodology}

Here is a way to add tables:
\begin{table}[H]
    \centering
    \begin{tabular}{lll}
               & Test Column 1 & Test Column 2 \\
    Test Row 1 & $1$           & $2$           \\
    Test Row 2 & $3$           & $4$          
    \end{tabular}
\end{table}
You can use this \href{https://www.tablesgenerator.com/}{website} to generate tables.

To add figures, you can read this \href{https://www.overleaf.com/learn/latex/Inserting_Images}{short guide} by Overleaf.

To add inline equations, you can do $f(x) = y$. To add line-broken equations, you can do
\[f(x) = y.\]
To add numbered equations, you can do
\begin{equation}
    f(x) = y.
\end{equation}
To add multiline equations, you can look up the \texttt{align} environment (and its unnumbered variant \texttt{align*}).

\section{Results}

\printbibliography

\end{document}

