{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "661cde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: torch==2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
      "Requirement already satisfied: jinja2 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.12.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.24.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.7.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/patrickbales/Library/Python/3.9/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f796aee",
   "metadata": {},
   "source": [
    "Detect if GPU available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f282b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab4c9a3",
   "metadata": {},
   "source": [
    "Load MNIST dataset of $28 \\times 28$ images. Training does NOT use the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "051e4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "87eea90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    '''\n",
    "    Visualizes IMG.\n",
    "    IMG should be a 2D torch Tensor.\n",
    "    '''\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3oepvCSLyfi"
   },
   "source": [
    "# Primal Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98adced1",
   "metadata": {},
   "source": [
    "This is the definition of the neural classifier for a custom number of layers (depth) and width. The first layer still has $28 \\times 28$ features, and the output layer still has ten output classes (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9cb6c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  \n",
    "  #Constructor with default NN width = 256 and default NN depth = 3\n",
    "  def __init__(self, width=256, depth=3):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.width = width\n",
    "    self.depth = depth\n",
    "    for w in range(0, depth - 1):\n",
    "      if w == 0:\n",
    "        self.layers.append(nn.Linear(in_features = 28*28, out_features = width))\n",
    "      elif w == depth - 2:\n",
    "        self.layers.append(nn.Linear(in_features = width, out_features = 10))   \n",
    "      else:\n",
    "        self.layers.append(nn.Linear(in_features = width, out_features = width))\n",
    "  \n",
    "  def forward(self, t):\n",
    "    '''\n",
    "    On input T, performs a affine transformation, then\n",
    "    a ReLU, then another affine transformation.\n",
    "    '''\n",
    "    self.z = []\n",
    "    \n",
    "    t = t.reshape(-1, 28*28)\n",
    "    for i in range(0, self.depth - 1):\n",
    "      t = self.layers[i](t)\n",
    "      self.z.append(t)\n",
    "      if i != self.depth - 2:\n",
    "        t = F.relu(t)\n",
    "    return t\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62d149ab",
   "metadata": {},
   "source": [
    "Provided training code using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8e70c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, trainloader, lr=0.001):\n",
    "    '''\n",
    "    Uses the Adam optimization algorithm to train \n",
    "    the classifier NET on training data from TRAINLOADER,\n",
    "    on loss function CRITERION, with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43d9cc3",
   "metadata": {},
   "source": [
    "Train the network using cross entropy loss. Note that this is equivalent to maximizing the KL-divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ea0df5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iter: 0 Loss 2.280247926712036\n",
      "Epoch 0 Iter: 500 Loss 0.8909095525741577\n",
      "Epoch 0 Iter: 1000 Loss 0.3296571373939514\n",
      "Epoch 0 Iter: 1500 Loss 0.015681760385632515\n",
      "Epoch 0 Iter: 2000 Loss 0.2830219268798828\n",
      "Epoch 0 Iter: 2500 Loss 0.16395944356918335\n",
      "Epoch 0 Iter: 3000 Loss 0.010710380971431732\n",
      "Epoch 0 Iter: 3500 Loss 0.08595582842826843\n",
      "Epoch 0 Iter: 4000 Loss 0.0006937599391676486\n",
      "Epoch 0 Iter: 4500 Loss 0.1510479599237442\n",
      "Epoch 0 Iter: 5000 Loss 0.47060954570770264\n",
      "Epoch 0 Iter: 5500 Loss 0.7411648631095886\n",
      "Epoch 0 Iter: 6000 Loss 0.12791958451271057\n",
      "Epoch 0 Iter: 6500 Loss 0.157392680644989\n",
      "Epoch 0 Iter: 7000 Loss 0.026938356459140778\n",
      "Epoch 0 Iter: 7500 Loss 0.1338925063610077\n",
      "Epoch 0 Iter: 8000 Loss 0.28727442026138306\n",
      "Epoch 0 Iter: 8500 Loss 0.004156887996941805\n",
      "Epoch 0 Iter: 9000 Loss 0.003123466856777668\n",
      "Epoch 0 Iter: 9500 Loss 0.018580088391900063\n",
      "Epoch 0 Iter: 10000 Loss 0.0002752102736849338\n",
      "Epoch 0 Iter: 10500 Loss 0.016523737460374832\n",
      "Epoch 0 Iter: 11000 Loss 0.004075531382113695\n",
      "Epoch 0 Iter: 11500 Loss 0.24522057175636292\n"
     ]
    }
   ],
   "source": [
    "net = Net(width=1000, depth=3)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(net, criterion, trainloader, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97fc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: tensor([[-38.8276, -19.7642, -13.7095, -18.2881, -19.3749, -27.6464, -64.1941,\n",
      "           3.2109, -26.1902, -13.2232]])\n",
      "Classifier prediction: 7\n"
     ]
    }
   ],
   "source": [
    "x, labels = next(test_iter)\n",
    "x = x[0].unsqueeze(0)\n",
    "labels = labels[0].unsqueeze(0)\n",
    "imshow(x[0,0])\n",
    "\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = net(x).data\n",
    "print('Classifier output:', out)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d65bfa74",
   "metadata": {},
   "source": [
    "Let's look at accuracy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, testloader):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET\n",
    "    on test data from TESTLOADER.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on original test dataset: 0.9627\n"
     ]
    }
   ],
   "source": [
    "print('Classifier accuracy on original test dataset:', accuracy(net, testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ceb29d",
   "metadata": {},
   "source": [
    "Implement FGSM Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(x, labels, net, eps):\n",
    "    '''\n",
    "    Given an input image X and its corresponding labels\n",
    "    LABELS, as well as a classifier NET, returns X\n",
    "    perturbed by EPS using the fast gradient sign method.\n",
    "    '''\n",
    "    net.zero_grad()    # Zero out any gradients from before\n",
    "    x.requires_grad=True    # Keep track of gradients\n",
    "    out = net(x)    # Output of classifier\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(out, labels)   # Classifier's loss\n",
    "    loss.backward()\n",
    "    grads = x.grad.data    # Gradient of loss w/r/t input\n",
    "    return x + eps*torch.sign(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661e90e",
   "metadata": {},
   "source": [
    "Now we define an epsilon value for FGSM perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a880d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier output: tensor([[-18.2339, -10.7099,  -5.0532,  -7.6614, -10.4145, -15.5425, -36.8277,\n",
      "           2.1194, -12.4814,  -6.0288]])\n",
      "Classifier prediction: 7\n"
     ]
    }
   ],
   "source": [
    "# We are using the same sample input x as before.\n",
    "x.requires_grad = True\n",
    "x_prime = FGSM(x, labels, net, eps)\n",
    "imshow(x_prime[0,0].cpu())\n",
    "out = net(x_prime)\n",
    "\n",
    "print('Classifier output:', out.data)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3de7e",
   "metadata": {},
   "source": [
    "From original assignment:\n",
    "We should evaluate the classifier's performance on FGSM-perturbed data by the same metric that we will later use in the primal adversarial problem. That is, for the classifier's output vector $\\vec{\\hat{z}}_3$, we want to compute\n",
    "$$\n",
    "\\vec{c}_j^\\top \\vec{\\hat{z}}_3\n",
    "$$\n",
    "where\n",
    "$$\\vec{c}_j={\\vec{y}_{\\text{true}}}-\\vec{e}_{j}$$\n",
    "for each $j\\in[10]$.\n",
    "\n",
    "Recall that \n",
    "$$\\vec{c}_j^\\top \\vec{\\hat{z}}_3=\\vec{\\hat{z}}_{3i_{\\text{true}}}-\\vec{\\hat{z}}_{3j},$$\n",
    "i.e. $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ is the difference between the classifier's confidence on the true class and the $j$th (incorrect) class. If $\\vec{c}_j^\\top \\vec{\\hat{z}}_3$ is positive for all incorrect $j$, then the classifier was not fooled by the adversarial perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e89790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20.35321807861328\n",
      "1 12.829313278198242\n",
      "2 7.1725616455078125\n",
      "3 9.780721664428711\n",
      "4 12.533903121948242\n",
      "5 17.66184425354004\n",
      "6 38.94705581665039\n",
      "8 14.600743293762207\n",
      "9 8.148200035095215\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, (out @ c).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_FGSM(net, testloader, eps):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET on test\n",
    "    data from TESTLOADER that has been perturbed by\n",
    "    EPS using FSGM.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        x, labels = data[0].to(device), data[1].to(device)\n",
    "        x_prime = FGSM(x, labels, net, eps)\n",
    "        outputs = net(x_prime)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on test dataset perturbed with FGSM: 0.8737\n"
     ]
    }
   ],
   "source": [
    "print('Classifier accuracy on test dataset perturbed with FGSM:', accuracy_on_FGSM(net, testloader, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d6284",
   "metadata": {},
   "source": [
    "# Dual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578abfa",
   "metadata": {},
   "source": [
    "From original assignment: $\\newline$\n",
    "Here, we will implement the dual network. First, we write the function to compute upper and lower bounds for the dual network. This function should take an input image, the trained classifier, and an epsilon value, and return the tuple\n",
    "$$(\\vec{l},\\vec{u},S,S^-,S^+)$$\n",
    "where $\\vec{u}$ and $\\vec{l}$ are the upper and lower bounds, respectively, for the input to the ReLU layer, and $S^-,S^+,S$ are sets defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&S:=\\{j\\in [n_2]\\mid l_{j}\\leq 0\\leq u_{j}\\}\\\\\n",
    "&S^{-}:=\\{j\\in [n_2]\\mid l_{j}\\leq u_{j}\\leq 0\\}\\\\\n",
    "&S^{+}:=\\{j\\in [n_2]\\mid 0\\leq l_{j}\\leq u_{j}\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See Section 6 of the PDF for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a885eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_bounds(x, net, eps):\n",
    "    '''\n",
    "    Given a classifier NET, an input image X,\n",
    "    and the epsilon parameter EPS, returns the lower\n",
    "    and upper bounds L and U respectively, as well as\n",
    "    the corresponding sets S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "\n",
    "    U = []\n",
    "    L = []\n",
    "    S = []\n",
    "    S_plus = []\n",
    "    S_min = []\n",
    "\n",
    "    for j in range(0, net.depth - 2):\n",
    "        print(j)\n",
    "        u = torch.Tensor([W[j][i] @ x + b[j][i] + eps * torch.norm(W[j][i], 1) for i in range(0, n)])\n",
    "        l = torch.Tensor([W[j][i] @ x + b[j][i] - eps * torch.norm(W[j][i], 1) for i in range(0, n)])\n",
    "        U.append(u)\n",
    "        L.append(l)\n",
    "\n",
    "        s = [i for i in range(0, n) if l[i] <= 0 and u[i] >= 0]\n",
    "        s_plus = [i for i in range(0, n) if l[i] <= u[i] and l[i] >= 0]\n",
    "        s_min = [i for i in range(0, n) if u[i] <= 0 and u[i] >= l[i]]\n",
    "        S.append(s)\n",
    "        S_plus.append(s_plus)\n",
    "        S_min.append(s_min)\n",
    "\n",
    "    return L, U, S, S_min, S_plus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53882d78",
   "metadata": {},
   "source": [
    "From original assignment: $\\newline$\n",
    "Given the tuple $(l,u,S,S^-,S^+)$, we are ready to calculate the dual objective itself. This function should take in an input image, the classifier, a vector $c$, and the $(l,u,S,S^-,S^+)$ from the previous function in order to output \n",
    "$$\n",
    "d^*(\\vec{x},\\vec{c})= \n",
    "-\\vec{\\hat{\\nu}}_1^\\top \\vec{x}-\\varepsilon\\|\\vec{\\hat{\\nu}}_1\\|_1-\\sum_{i=1}^{2}\\vec{\\nu}_{i+1}^\\top \\vec{b}_i+\\sum_{j\\in S\\\n",
    "}l_{j}\\text{ReLU}(\\nu_{2j})\n",
    "$$\n",
    "\n",
    "Where the $\\vec{\\nu}$ vectors are computed as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\vec{\\nu}_3=-\\vec{c}\\\\\n",
    "&\\vec{\\hat{\\nu}}_2=W_2^\\top \\vec{\\nu}_{3}\\\\\n",
    "&\\nu_{2j}=0 && \\forall j\\in S^-\\\\\n",
    "&\\nu_{2j}=\\hat{\\nu}_{2j} && \\forall j\\in S^+\\\\\n",
    "&\\nu_{2j}=\\dfrac{u_{j}}{u_{j}-l_{j}}\\hat{\\nu}_{2j} && \\forall j\\in S\\\\\n",
    "&\\vec{\\hat{\\nu}}_1=W_1^\\top \\vec{\\nu_{2}}\n",
    "&\\end{aligned}.\n",
    "$$\n",
    "\n",
    "Again, see Section 6 of the PDF for more details.\n",
    "\n",
    "One efficient way to compute $\\vec{\\nu}_2$ is to rewrite it as\n",
    "$$\\vec{\\nu}_2= D\\vec{\\hat{\\nu}}_2,$$\n",
    "where $D$ is a diagonal matrix defined  by\n",
    "$$\n",
    "D_{jj}=\\begin{cases}\n",
    "0 & j\\in S^-\\\\\n",
    "\\hat{\\nu}_{2j} & j\\in S^+\\\\\n",
    "\\dfrac{u_{j}}{u_{j}-l_{j}}\\hat{\\nu}_{2j} & j\\in S.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the diagonal D matrix from the S sets, n (the dimensionality\n",
    "# of the hidden layer), u, and l.\n",
    "def StoD(S_min, S_plus, S, n, U, L):\n",
    "    '''\n",
    "    Given upper and lower bounds U and L, as well\n",
    "    as the corresponding sets S_MIN, S_PLUS, and S,\n",
    "    as well as the dimension of the hidden layer N,\n",
    "    returns the corresponding diagonal matrix D.\n",
    "    '''\n",
    "    D = []\n",
    "    for i in range(0, net.depth - 2):\n",
    "        d = []\n",
    "        for j in range(n):\n",
    "            if j in S[i]:\n",
    "                d.append((U[i][j] / (U[i][j] - L[i][j])).item())\n",
    "            elif j in S_plus[i]:\n",
    "                d.append(1)\n",
    "            elif j in S_min[i]:\n",
    "                d.append(0)\n",
    "            else:\n",
    "                assert False, 'StoD error.'\n",
    "        D.append(torch.diag(torch.Tensor(d)).to(device))\n",
    "    return D\n",
    "\n",
    "def dual_forward(x, net, c, eps, L, U, S, S_min, S_plus):\n",
    "    '''\n",
    "    Calculates the dual objective for classifier NET with input X\n",
    "    and dual input C and epsilon parameter S. Depends on lower\n",
    "    and upper bounds L and U, as well as the corresponding sets\n",
    "    S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "    D = StoD(S_min, S_plus, S, n, U, L)\n",
    "    nu = []\n",
    "    nuh = []\n",
    "\n",
    "    for i in range(0, net.depth - 1):\n",
    "        if i == 0:\n",
    "            nu.append(-c)\n",
    "        else:\n",
    "            nu.append(D[net.depth - 2 - i] @ nuh[i - 1])\n",
    "        nuh.append(W[net.depth - 2 - i].T @ nu[i])\n",
    "    \n",
    "    nuh = nuh[::-1]\n",
    "    nu = nu[::-1]\n",
    "    \n",
    "    nu_sum = 0\n",
    "    for i in range(1, net.depth - 1):\n",
    "        nu_sum += nu[i].T @ b[i]\n",
    "    \n",
    "    relu_sum = 0\n",
    "    for i in range(0, net.depth - 2):\n",
    "        for j in range(len(nu[i])):\n",
    "            if j in S[i]:\n",
    "                relu_sum += L[i][j] * torch.relu(nu[i][j])\n",
    "    dual = -1 * (nuh[0].T @ x) - eps * (torch.norm(nuh[0], 1)) - nu_sum + relu_sum\n",
    "\n",
    "    return dual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x256 and 784x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb Cell 34\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We are still using the same sample input x as before.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m L, U, S, S_min, S_plus \u001b[39m=\u001b[39m dual_bounds(x, net, eps)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print(l, u, S, S_min, S_plus)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Here, we loop through each column c_j defined above, and output the \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# objective value for the dual function with input c.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n",
      "\u001b[1;32m/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb Cell 34\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, net\u001b[39m.\u001b[39mdepth \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(j)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     u \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([W[j][i] \u001b[39m@\u001b[39m x \u001b[39m+\u001b[39m b[j][i] \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(W[j][i], \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([W[j][i] \u001b[39m@\u001b[39m x \u001b[39m+\u001b[39m b[j][i] \u001b[39m-\u001b[39m eps \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(W[j][i], \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     U\u001b[39m.\u001b[39mappend(u)\n",
      "\u001b[1;32m/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb Cell 34\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, net\u001b[39m.\u001b[39mdepth \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(j)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     u \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([W[j][i] \u001b[39m@\u001b[39;49m x \u001b[39m+\u001b[39m b[j][i] \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(W[j][i], \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     l \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor([W[j][i] \u001b[39m@\u001b[39m x \u001b[39m+\u001b[39m b[j][i] \u001b[39m-\u001b[39m eps \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mnorm(W[j][i], \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrickbales/Desktop/GitHub/AdversarialML/Empirical_Analysis_of_CNN_Robustness_Properties.ipynb#Y200sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     U\u001b[39m.\u001b[39mappend(u)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x256 and 784x1)"
     ]
    }
   ],
   "source": [
    "# We are still using the same sample input x as before.\n",
    "L, U, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "\n",
    "# print(l, u, S, S_min, S_plus)\n",
    "# Here, we loop through each column c_j defined above, and output the \n",
    "# objective value for the dual function with input c.\n",
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, dual_forward(x, net, c, eps, L, U, S, S_min, S_plus).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b801c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_loss(x, label, net, eps, criterion):\n",
    "    '''\n",
    "    Given a batch of input images X, its corresponding lables LABEL,\n",
    "    the classifier NET, epsilon value EPS, and original loss\n",
    "    function CRITERION, returns the robust loss of NET w/r/t\n",
    "    the original loss function, on the input image.\n",
    "    '''\n",
    "    l, u, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "    # We assume there are 10 classes.\n",
    "    e_y = torch.zeros(10, 1)\n",
    "    e_y[label] = 1\n",
    "    c = e_y @ torch.ones(1, 10) - torch.eye(10)\n",
    "    J = dual_forward(x, net, c, eps, l, u, S, S_min, S_plus).unsqueeze(0)\n",
    "    return criterion(-J, label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e25dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_train(net, criterion, trainloader, eps, lr=0.001):\n",
    "    '''\n",
    "    Trains the classifier NET using the robust version\n",
    "    of the original loss function CRITERION with paramater EPS,\n",
    "    using training data from TRAINLOADER and with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for i in range(inputs.shape[0]):\n",
    "                x = inputs[i].unsqueeze(0)\n",
    "                label = labels[i].unsqueeze(0)\n",
    "                loss += robust_loss(x, label, net, eps, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603259b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
