{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "661cde4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.9/site-packages (from torch) (1.9)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.9/site-packages (0.15.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: torch==2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.0.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.3.1)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (1.9)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.10.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchvision) (1.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision) (1.2.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.9/site-packages (1.22.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import copy\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f796aee",
   "metadata": {},
   "source": [
    "Detect if GPU available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f282b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab4c9a3",
   "metadata": {},
   "source": [
    "Load MNIST dataset of $28 \\times 28$ images. Training does NOT use the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051e4c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87eea90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    '''\n",
    "    Visualizes IMG.\n",
    "    IMG should be a 2D torch Tensor.\n",
    "    '''\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4409f781",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3oepvCSLyfi"
   },
   "source": [
    "# Primal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98adced1",
   "metadata": {},
   "source": [
    "This is the definition of the neural classifier for a custom number of layers (depth) and width. The first layer has $28 \\times 28$ features, and the output layer has ten output classes (0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb6c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  \n",
    "  #Constructor with default NN width = 256 and default NN depth = 3\n",
    "  def __init__(self, width=256, depth=3):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.width = width\n",
    "    self.depth = depth\n",
    "    for w in range(0, depth - 1):\n",
    "      if w == 0:\n",
    "        self.layers.append(nn.Linear(in_features = 28*28, out_features = width))\n",
    "      elif w == depth - 2:\n",
    "        self.layers.append(nn.Linear(in_features = width, out_features = 10))   \n",
    "      else:\n",
    "        self.layers.append(nn.Linear(in_features = width, out_features = width))\n",
    "  \n",
    "  def forward(self, t):\n",
    "    '''\n",
    "    On input T, performs a affine transformation, then\n",
    "    a ReLU, then another affine transformation.\n",
    "    '''\n",
    "    self.z = []\n",
    "    \n",
    "    t = t.reshape(-1, 28*28)\n",
    "    for i in range(0, self.depth - 1):\n",
    "      t = self.layers[i](t)\n",
    "      self.z.append(t)\n",
    "      if i != self.depth - 2:\n",
    "        t = F.relu(t)\n",
    "    return t\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d149ab",
   "metadata": {},
   "source": [
    "Provided training code using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, trainloader, lr=0.001):\n",
    "    '''\n",
    "    Uses the Adam optimization algorithm to train \n",
    "    the classifier NET on training data from TRAINLOADER,\n",
    "    on loss function CRITERION, with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d9cc3",
   "metadata": {},
   "source": [
    "Train the network using cross entropy loss. Note that this is equivalent to maximizing the KL-divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0df5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iter: 0 Loss 2.3998305797576904\n",
      "Epoch 0 Iter: 500 Loss 0.34705400466918945\n",
      "Epoch 0 Iter: 1000 Loss 0.47266754508018494\n",
      "Epoch 0 Iter: 1500 Loss 1.5026980638504028\n",
      "Epoch 0 Iter: 2000 Loss 0.2118740975856781\n",
      "Epoch 0 Iter: 2500 Loss 0.40300479531288147\n",
      "Epoch 0 Iter: 3000 Loss 0.6526918411254883\n",
      "Epoch 0 Iter: 3500 Loss 0.21472741663455963\n",
      "Epoch 0 Iter: 4000 Loss 0.2304287999868393\n",
      "Epoch 0 Iter: 4500 Loss 0.04490993171930313\n",
      "Epoch 0 Iter: 5000 Loss 0.5661467909812927\n",
      "Epoch 0 Iter: 5500 Loss 0.4571095108985901\n",
      "Epoch 0 Iter: 6000 Loss 0.010872064158320427\n",
      "Epoch 0 Iter: 6500 Loss 0.0032171003986150026\n",
      "Epoch 0 Iter: 7000 Loss 0.02653050422668457\n",
      "Epoch 0 Iter: 7500 Loss 0.0065382421016693115\n",
      "Epoch 0 Iter: 8000 Loss 0.007696693763136864\n",
      "Epoch 0 Iter: 8500 Loss 0.3958502411842346\n",
      "Epoch 0 Iter: 9000 Loss 0.04818666726350784\n",
      "Epoch 0 Iter: 9500 Loss 0.35107874870300293\n",
      "Epoch 0 Iter: 10000 Loss 0.2825327515602112\n",
      "Epoch 0 Iter: 10500 Loss 0.00436755595728755\n",
      "Epoch 0 Iter: 11000 Loss 0.3105979859828949\n",
      "Epoch 0 Iter: 11500 Loss 0.009252277202904224\n",
      "Epoch 0 Iter: 12000 Loss 0.1005420833826065\n",
      "Epoch 0 Iter: 12500 Loss 0.032086361199617386\n",
      "Epoch 0 Iter: 13000 Loss 0.2702230215072632\n",
      "Epoch 0 Iter: 13500 Loss 0.005098570603877306\n",
      "Epoch 0 Iter: 14000 Loss 1.4407804012298584\n",
      "Epoch 0 Iter: 14500 Loss 0.05169522017240524\n",
      "Epoch 1 Iter: 0 Loss 0.12493251264095306\n",
      "Epoch 1 Iter: 500 Loss 0.15362538397312164\n",
      "Epoch 1 Iter: 1000 Loss 0.10460548102855682\n",
      "Epoch 1 Iter: 1500 Loss 0.010852005332708359\n",
      "Epoch 1 Iter: 2000 Loss 0.04144715890288353\n",
      "Epoch 1 Iter: 2500 Loss 0.017628923058509827\n",
      "Epoch 1 Iter: 3000 Loss 0.002472609980031848\n",
      "Epoch 1 Iter: 3500 Loss 0.024806667119264603\n",
      "Epoch 1 Iter: 4000 Loss 0.0004322666791267693\n",
      "Epoch 1 Iter: 4500 Loss 0.018841814249753952\n",
      "Epoch 1 Iter: 5000 Loss 0.3950296640396118\n",
      "Epoch 1 Iter: 5500 Loss 0.002570580691099167\n",
      "Epoch 1 Iter: 6000 Loss 8.66825066623278e-05\n",
      "Epoch 1 Iter: 6500 Loss 0.011931706219911575\n",
      "Epoch 1 Iter: 7000 Loss 0.04044310003519058\n",
      "Epoch 1 Iter: 7500 Loss 0.6491401195526123\n",
      "Epoch 1 Iter: 8000 Loss 0.06410958617925644\n",
      "Epoch 1 Iter: 8500 Loss 0.050564661622047424\n",
      "Epoch 1 Iter: 9000 Loss 0.0732756033539772\n",
      "Epoch 1 Iter: 9500 Loss 0.00033814163180068135\n",
      "Epoch 1 Iter: 10000 Loss 0.0006546921795234084\n",
      "Epoch 1 Iter: 10500 Loss 0.0006371220224536955\n",
      "Epoch 1 Iter: 11000 Loss 0.05149780958890915\n",
      "Epoch 1 Iter: 11500 Loss 0.03194064274430275\n",
      "Epoch 1 Iter: 12000 Loss 0.03738851845264435\n",
      "Epoch 1 Iter: 12500 Loss 0.0043157474137842655\n",
      "Epoch 1 Iter: 13000 Loss 0.06822650879621506\n",
      "Epoch 1 Iter: 13500 Loss 0.004010925069451332\n",
      "Epoch 1 Iter: 14000 Loss 0.005667935125529766\n",
      "Epoch 1 Iter: 14500 Loss 0.13056127727031708\n",
      "Epoch 2 Iter: 0 Loss 0.016193488612771034\n",
      "Epoch 2 Iter: 500 Loss 0.013674125075340271\n",
      "Epoch 2 Iter: 1000 Loss 0.0003940693859476596\n",
      "Epoch 2 Iter: 1500 Loss 0.0002832003519870341\n",
      "Epoch 2 Iter: 2000 Loss 0.05719888210296631\n",
      "Epoch 2 Iter: 2500 Loss 0.4533081352710724\n",
      "Epoch 2 Iter: 3000 Loss 0.8663992285728455\n",
      "Epoch 2 Iter: 3500 Loss 0.007507104426622391\n",
      "Epoch 2 Iter: 4000 Loss 0.02122880518436432\n",
      "Epoch 2 Iter: 4500 Loss 0.0447990857064724\n",
      "Epoch 2 Iter: 5000 Loss 0.9241946935653687\n",
      "Epoch 2 Iter: 5500 Loss 0.000839382060803473\n",
      "Epoch 2 Iter: 6000 Loss 0.0008301868219859898\n",
      "Epoch 2 Iter: 6500 Loss 0.000222393122385256\n",
      "Epoch 2 Iter: 7000 Loss 0.018685849383473396\n",
      "Epoch 2 Iter: 7500 Loss 0.025024214759469032\n",
      "Epoch 2 Iter: 8000 Loss 0.008063137531280518\n",
      "Epoch 2 Iter: 8500 Loss 0.0015478639397770166\n",
      "Epoch 2 Iter: 9000 Loss 0.0010327404597774148\n",
      "Epoch 2 Iter: 9500 Loss 1.4985343217849731\n",
      "Epoch 2 Iter: 10000 Loss 0.37769871950149536\n",
      "Epoch 2 Iter: 10500 Loss 0.0008162383455783129\n",
      "Epoch 2 Iter: 11000 Loss 0.0002111375069944188\n",
      "Epoch 2 Iter: 11500 Loss 0.868233323097229\n",
      "Epoch 2 Iter: 12000 Loss 0.011608961038291454\n",
      "Epoch 2 Iter: 12500 Loss 0.003188604721799493\n",
      "Epoch 2 Iter: 13000 Loss 0.09994093328714371\n",
      "Epoch 2 Iter: 13500 Loss 6.729013693984598e-05\n",
      "Epoch 2 Iter: 14000 Loss 0.3061870038509369\n",
      "Epoch 2 Iter: 14500 Loss 0.0017136905808001757\n"
     ]
    }
   ],
   "source": [
    "net = Net(width=150, depth=3)\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(net, criterion, trainloader, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, labels = next(test_iter)\n",
    "x = x[0].unsqueeze(0)\n",
    "labels = labels[0].unsqueeze(0)\n",
    "imshow(x[0,0])\n",
    "\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = net(x).data\n",
    "print('Classifier output:', out)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65bfa74",
   "metadata": {},
   "source": [
    "Let's look at accuracy now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, testloader):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET\n",
    "    on test data from TESTLOADER.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d681f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classifier accuracy on original test dataset:', accuracy(net, testloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ceb29d",
   "metadata": {},
   "source": [
    "Implement FGSM Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGSM(x, labels, net, eps):\n",
    "    '''\n",
    "    Given an input image X and its corresponding labels\n",
    "    LABELS, as well as a classifier NET, returns X\n",
    "    perturbed by EPS using the fast gradient sign method.\n",
    "    '''\n",
    "    net.zero_grad()    # Zero out any gradients from before\n",
    "    x.requires_grad=True    # Keep track of gradients\n",
    "    out = net(x)    # Output of classifier\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(out, labels)   # Classifier's loss\n",
    "    loss.backward()\n",
    "    grads = x.grad.data    # Gradient of loss w/r/t input\n",
    "    return x + eps*torch.sign(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661e90e",
   "metadata": {},
   "source": [
    "Now we define an epsilon value for FGSM perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a880d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same sample input x as before.\n",
    "x.requires_grad = True\n",
    "x_prime = FGSM(x, labels, net, eps)\n",
    "imshow(x_prime[0,0].cpu())\n",
    "out = net(x_prime)\n",
    "\n",
    "print('Classifier output:', out.data)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e89790",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, (out @ c).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_FGSM(net, testloader, eps):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET on test\n",
    "    data from TESTLOADER that has been perturbed by\n",
    "    EPS using FSGM.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        x, labels = data[0].to(device), data[1].to(device)\n",
    "        x_prime = FGSM(x, labels, net, eps)\n",
    "        outputs = net(x_prime)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classifier accuracy on test dataset perturbed with FGSM:', accuracy_on_FGSM(net, testloader, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d6284",
   "metadata": {},
   "source": [
    "# Dual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a885eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_bounds(x, net, eps):\n",
    "    '''\n",
    "    Given a classifier NET, an input image X,\n",
    "    and the epsilon parameter EPS, returns the lower\n",
    "    and upper bounds L and U respectively, as well as\n",
    "    the corresponding sets S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "\n",
    "    U = []\n",
    "    L = []\n",
    "    S = []\n",
    "    S_plus = []\n",
    "    S_min = []\n",
    "\n",
    "    for j in range(0, net.depth - 2):\n",
    "\n",
    "        u = torch.Tensor([W[0][i] @ x + b[j][i] + eps * torch.norm(W[0][i], 1) for i in range(0, n)])\n",
    "        l = torch.Tensor([W[0][i] @ x + b[j][i] - eps * torch.norm(W[0][i], 1) for i in range(0, n)])\n",
    "        #u = torch.Tensor([W[j][i] @ net.z[j-1] + b[j][i] + eps * torch.norm(W[j][i], 1) for i in range(0, n)])\n",
    "        #l = torch.Tensor([W[j][i] @ net.z[j-1] + b[j][i] - eps * torch.norm(W[j][i], 1) for i in range(0, n)])\n",
    "        \n",
    "        U.append(u)\n",
    "        L.append(l)\n",
    "\n",
    "        s = [i for i in range(0, n) if l[i] <= 0 and u[i] >= 0]\n",
    "        s_plus = [i for i in range(0, n) if l[i] <= u[i] and l[i] >= 0]\n",
    "        s_min = [i for i in range(0, n) if u[i] <= 0 and u[i] >= l[i]]\n",
    "        S.append(s)\n",
    "        S_plus.append(s_plus)\n",
    "        S_min.append(s_min)\n",
    "\n",
    "    return L, U, S, S_min, S_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53882d78",
   "metadata": {},
   "source": [
    "Given the tuple $(l,u,S,S^-,S^+)$, we are ready to calculate the dual objective itself. This function should take in an input image, the classifier, a vector $c$, and the $(l,u,S,S^-,S^+)$ from the previous function in order to output \n",
    "$$\n",
    "d^*(\\vec{x}, \\vec{c}) = \\max_{\\vec{\\nu}}(-\\vec{\\hat{\\nu}}_1^T\\vec{x}-\\epsilon||\\vec{\\hat{\\nu}}_1||_1-\\sum_{i=1}^{n-1}\\vec{\\nu}_{i+1}^T\\vec{b}_i+\\sum_{i=2}^{n-1}\\sum_{j \\in S_i}l_{ij}ReLU(\\vec{\\nu}_{ij}))\n",
    "$$\n",
    "\n",
    "One efficient way to compute $\\vec{\\nu}_i$ is to rewrite it as\n",
    "$$\\vec{\\nu}_i= D\\vec{\\hat{\\nu}}_i,$$\n",
    "where $D$ is a diagonal matrix defined  by\n",
    "$$\n",
    "D_{jj}=\\begin{cases}\n",
    "0 & j\\in S^-\\\\\n",
    "\\hat{\\nu}_{ij} & j\\in S^+\\\\\n",
    "\\dfrac{u_{j}}{u_{j}-l_{j}}\\hat{\\nu}_{ij} & j\\in S.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs the diagonal D matrix from the S sets, n (the dimensionality\n",
    "# of the hidden layer), u, and l.\n",
    "def StoD(S_min, S_plus, S, n, U, L):\n",
    "    '''\n",
    "    Given upper and lower bounds U and L, as well\n",
    "    as the corresponding sets S_MIN, S_PLUS, and S,\n",
    "    as well as the dimension of the hidden layer N,\n",
    "    returns the corresponding diagonal matrix D.\n",
    "    '''\n",
    "    D = []\n",
    "    for i in range(0, net.depth - 2):\n",
    "        d = []\n",
    "        for j in range(n):\n",
    "            if j in S[i]:\n",
    "                d.append((U[i][j] / (U[i][j] - L[i][j])).item())\n",
    "            elif j in S_plus[i]:\n",
    "                d.append(1)\n",
    "            elif j in S_min[i]:\n",
    "                d.append(0)\n",
    "            else:\n",
    "                assert False, 'StoD error.'\n",
    "        D.append(torch.diag(torch.Tensor(d)).to(device))\n",
    "    return D\n",
    "\n",
    "def dual_forward(x, net, c, eps, L, U, S, S_min, S_plus):\n",
    "    '''\n",
    "    Calculates the dual objective for classifier NET with input X\n",
    "    and dual input C and epsilon parameter S. Depends on lower\n",
    "    and upper bounds L and U, as well as the corresponding sets\n",
    "    S, S_MIN, S_PLUS.\n",
    "    '''\n",
    "    x = x[0].reshape(-1, 1)    # Reshape input to more convenient dimensions\n",
    "    W = [layer.weight for layer in net.layers]    # Array of network weights (W matrices)\n",
    "    b = [layer.bias.reshape(-1, 1) for layer in net.layers]    # Array of network biases (b vectors)\n",
    "    n = W[1].shape[1]    # Dimensionality of hidden layer\n",
    "    D = StoD(S_min, S_plus, S, n, U, L)\n",
    "    nu = []\n",
    "    nuh = []\n",
    "\n",
    "    for i in range(0, net.depth - 1):\n",
    "        if i == 0:\n",
    "            nu.append(-c)\n",
    "        else:\n",
    "            nu.append(D[net.depth - 2 - i] @ nuh[i - 1])\n",
    "        nuh.append(W[net.depth - 2 - i].T @ nu[i])\n",
    "    \n",
    "    nuh = nuh[::-1]\n",
    "    nu = nu[::-1]\n",
    "    \n",
    "    nu_sum = 0\n",
    "    for i in range(1, net.depth - 1):\n",
    "        nu_sum += nu[i].T @ b[i]\n",
    "    \n",
    "    relu_sum = 0\n",
    "    for i in range(0, net.depth - 2):\n",
    "        for j in range(len(nu[i])):\n",
    "            if j in S[i]:\n",
    "                relu_sum += L[i][j] * torch.relu(nu[i][j])\n",
    "    dual = -1 * (nuh[0].T @ x) - eps * (torch.norm(nuh[0], 1)) - nu_sum + relu_sum\n",
    "\n",
    "    return dual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are still using the same sample input x as before.\n",
    "L, U, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "\n",
    "# print(l, u, S, S_min, S_plus)\n",
    "# Here, we loop through each column c_j defined above, and output the \n",
    "# objective value for the dual function with input c.\n",
    "for i in range(10):\n",
    "    c = torch.zeros(10, 1).to(device)\n",
    "    if i != labels:\n",
    "        c[i] = -1\n",
    "        c[labels] = 1\n",
    "        print(i, dual_forward(x, net, c, 0.1, L, U, S, S_min, S_plus).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b801c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_loss(x, label, net, eps, criterion):\n",
    "    '''\n",
    "    Given a batch of input images X, its corresponding lables LABEL,\n",
    "    the classifier NET, epsilon value EPS, and original loss\n",
    "    function CRITERION, returns the robust loss of NET w/r/t\n",
    "    the original loss function, on the input image.\n",
    "    '''\n",
    "    l, u, S, S_min, S_plus = dual_bounds(x, net, eps)\n",
    "    # We assume there are 10 classes.\n",
    "    e_y = torch.zeros(10, 1)\n",
    "    e_y[label] = 1\n",
    "    c = e_y @ torch.ones(1, 10) - torch.eye(10)\n",
    "    J = dual_forward(x, net, c, 0.1, l, u, S, S_min, S_plus).unsqueeze(0)\n",
    "    try:\n",
    "        temp = criterion(-J, label.unsqueeze(0))\n",
    "    except:\n",
    "        temp = criterion(-J, label.unsqueeze(0).expand(1,10))\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e25dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_train(net, criterion, trainloader, eps, lr=0.001):\n",
    "    '''\n",
    "    Trains the classifier NET using the robust version\n",
    "    of the original loss function CRITERION with paramater EPS,\n",
    "    using training data from TRAINLOADER and with learning rate LR.\n",
    "    \n",
    "    Note that we half the learning rate each epoch.\n",
    "    '''\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        \n",
    "    for epoch in range(1):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr * 0.5 ** (epoch)\n",
    "\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0.0\n",
    "            for k in range(inputs.shape[0]):\n",
    "                x = inputs[k].unsqueeze(0)\n",
    "                label = labels[k].unsqueeze(0)\n",
    "                loss += robust_loss(x, label, net, eps, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 500 == 0:\n",
    "                print('Epoch', epoch, 'Iter:', i, 'Loss', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603259b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(width=150, depth=3)\n",
    "net.to(device)\n",
    "\n",
    "eps = 0.05\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "robust_train(net, criterion, trainloader, eps, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0648b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cf4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, labels = next(test_iter)\n",
    "x = x[0].unsqueeze(0)\n",
    "labels = labels[0].unsqueeze(0)\n",
    "imshow(x[0,0])\n",
    "\n",
    "x = x.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = net(x).data\n",
    "print('Classifier output:', out)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6c6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the same sample input x as before.\n",
    "x.requires_grad = True\n",
    "x_prime = FGSM(x, labels, net, eps)\n",
    "imshow(x_prime[0,0].cpu())\n",
    "out = net(x_prime)\n",
    "\n",
    "print('Classifier output:', out.data)\n",
    "print('Classifier prediction:', torch.argmax(out).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3623c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(net, testloader):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET\n",
    "    on test data from TESTLOADER.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classifier accuracy on original test dataset:', accuracy(net, testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa645196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_on_FGSM(net, testloader, eps):\n",
    "    '''\n",
    "    Returns the accuracy of classifier NET on test\n",
    "    data from TESTLOADER that has been perturbed by\n",
    "    EPS using FSGM.\n",
    "    '''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in testloader:\n",
    "        x, labels = data[0].to(device), data[1].to(device)\n",
    "        x_prime = FGSM(x, labels, net, eps)\n",
    "        outputs = net(x_prime)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9596d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classifier accuracy on test dataset perturbed with FGSM:', accuracy_on_FGSM(net, testloader, eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gathered from multiple iterations, updated with each re-run of this script.  Plot below measures classifier accuracy on test dataset perturbed with FGSM \n",
    "# relative to width compared to a baseline of width 256 and depth 3.  All runs were with a network of depth 3.\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "\n",
    "depth_data = [3, 4, 5, 6]\n",
    "robust_data = [0.83, 0.54, 0.47, 0.38]\n",
    "original_data = [0.89, 0.84, 0.86, 0.80]\n",
    "\n",
    "plt.xlim(2.5, 6.5)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.plot(depth_data, robust_data)\n",
    "plt.plot(depth_data, original_data)\n",
    "plt.axhline(y=0.89, color='r')\n",
    "\n",
    "plt.legend(['Variable Depth Robust Loss', 'Variable Depth Original Loss', 'Original Forward Loss Baseline (Depth=3)'])\n",
    "plt.title(\"Classifier Accuracy on Test Dataset Perturbed with FGSM \\nUsing Robust Loss vs. Network Depth with Width=256\\n\")\n",
    "plt.xlabel(\"Network Depth (Layers)\")\n",
    "plt.ylabel(\"Classifier Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gathered from multiple iterations, updated with each re-run of this script.  Plot below measures classifier accuracy on test dataset perturbed with FGSM \n",
    "# relative to width compared to a baseline of width 256 and depth 3.  All runs were with a network of depth 3.\n",
    "\n",
    "fig1 = plt.figure(figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "\n",
    "depth_data = [150, 256, 512, 750]\n",
    "robust_data = [0.83, 0.61, 0.47, 0.32]\n",
    "original_data = [0.89, 0.84, 0.86, 0.80]\n",
    "\n",
    "plt.xlim(200, 1150)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.plot(depth_data, robust_data)\n",
    "plt.plot(depth_data, original_data)\n",
    "plt.axhline(y=0.89, color='r')\n",
    "\n",
    "plt.legend(['Variable Width Robust Loss', 'Variable Width Original Loss', 'Original Forward Loss Baseline (Width=256)'])\n",
    "plt.title(\"Classifier Accuracy on Test Dataset Perturbed with FGSM \\nUsing Robust Loss vs. Network Width with Depth=3\\n\")\n",
    "plt.xlabel(\"Network Width (Nodes/Hidden Layer)\")\n",
    "plt.ylabel(\"Classifier Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39695f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
